//! Inference service â€” bridges HTTP requests to llama-core generation.
//!
//! Phase 1: thin wrapper. Phase 2 will add request queuing,
//! slot management, and multi-model routing.
