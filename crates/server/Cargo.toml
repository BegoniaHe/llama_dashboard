[package]
name = "llama-dashboard"
version.workspace = true
edition.workspace = true
license.workspace = true
description = "Local LLM management platform powered by llama.cpp"

[[bin]]
name = "llama-dashboard"
path = "src/main.rs"

[dependencies]
llama-core = { workspace = true }
gguf-parser = { workspace = true }

# Async runtime
tokio = { workspace = true }

# Web framework
axum = { version = "0.8", features = ["ws"] }
tower = "0.5"
tower-http = { version = "0.6", features = ["cors"] }

# Serialization
serde = { workspace = true }
serde_json = { workspace = true }

# CLI
clap = { version = "4", features = ["derive", "env"] }

# Database
rusqlite = { version = "0.32", features = ["bundled"] }

# Logging
tracing = { workspace = true }
tracing-subscriber = { workspace = true }

# Error handling
anyhow = { workspace = true }
thiserror = { workspace = true }

# Utilities
uuid = { version = "1", features = ["v4"] }
chrono = { version = "0.4", features = ["serde"] }
dirs = "6"
tokio-stream = "0.1"

# Frontend embedding (optional â€“ only needed when frontend/dist exists)
rust-embed = { version = "8", features = ["compression"], optional = true }
mime_guess = { version = "2", optional = true }

# WebSocket
futures-util = "0.3"

[features]
default = ["embed-frontend"]
embed-frontend = ["rust-embed", "mime_guess"]
cuda = ["llama-core/cuda"]
vulkan = ["llama-core/vulkan"]
rocm = ["llama-core/rocm"]

[lints.rust]
unexpected_cfgs = "allow"
